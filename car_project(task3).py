# -*- coding: utf-8 -*-
"""Car_Project(Task3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-_-Hg90U_QMYRW8o5smNdWNGwnDLdWNf

**Project Overview:**

In this project,I aim to build a Car Price Prediction Model using Machine Learning (ML) techniques. The goal is to predict the selling price of a car based on various attributes such as the car's brand, year, mileage, fuel type, transmission, and more. The project also includes the creation of an interactive Dash web application, where users can input the car's characteristics and receive the predicted price.

**Dataset Overview**

1. Car_Name: Categorical variable representing the model of the vehicle, including both cars and motorcycles.
2. Year: Numerical (integer) indicating the manufacturing year, ranging from 2003 to 2018.
3. Selling_Price: Numerical (float) representing the price at which the vehicle is being sold.
4. Present_Price: Numerical (float) indicating the current market value of the vehicle.
5. Driven_kms: Numerical (integer) showing the kilometers driven.
6. Fuel_Type: Categorical variable indicating the type of fuel used (e.g., Petrol, Diesel, CNG).
7. Selling_type: Categorical variable denoting whether the vehicle is sold by a Dealer or an Individual.
8. Transmission: Categorical variable indicating the type of transmission:
        *. Manual: The driver manually changes gears using a gear stick and clutch pedal.
       *.  Automatic: The car automatically changes gears without driver intervention.
9. Owner: Numerical (integer) indicating the number of previous owners the vehicle has had.
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Step 1: Setup Environment**"""

!pip install dash

"""**Step 2: Import Libraries**"""

# Data processing and model building
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
from sklearn.metrics import make_scorer

# Dash libraries for building the web application
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.express as px

"""**Step 3: Load and Explore the Dataset**"""

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/car data.csv')

# Check the first few rows of the dataset
# This will display the first 5 rows of the dataset
print("First 5 rows of the dataset:")
print(data.head())

# Check the shape of the dataset
# Shape will give us the number of rows and columns (rows, columns)
print(f"\nShape of the dataset (rows, columns): {data.shape}")

# Check for missing values
# This will display the number of missing values in each column
print("\nMissing values in each column:")
print(data.isnull().sum())

# Check for duplicates
print("\nNumber of duplicate rows before cleaning:")
print(data.duplicated().sum())

# Data types of columns
print("\nData Types of Columns:")
print(data.dtypes)

# Information about the dataset (includes data types, non-null counts, and memory usage)
print("\nInformation about the dataset:")
print(data.info())

# Summary statistics of numerical columns
# This will give you the mean, standard deviation, min, max, and other statistics for numerical columns
print("\nSummary statistics of numerical columns:")
data.describe().T

"""**Step 4: Get Unique Values in Each Column**"""

# Loop through each column and print unique values
for column in data.columns:
    print(f"Unique values in column '{column}':")
    print(data[column].unique())
    print("\n")

"""**Step 4: Data Cleaning**"""

# Convert 'Year' to datetime type (representing the first day of each year)
data['Year'] = pd.to_datetime(data['Year'], format='%Y')

# Display the updated 'Year' column
print("\nUpdated 'Year' column:")
print(data['Year'].head())

# Drop duplicates
data = data.drop_duplicates()

# Check for duplicates again and count the number of duplicates
duplicate_count = data.duplicated().sum()
print(f"\nNumber of duplicate rows after cleaning: {duplicate_count}")

"""**Detecting and Visualizing Outliers**"""

# Function to create boxplots for numerical columns
def plot_boxplot(data, numerical_columns, title_prefix=""):
    # Set up the subplots (grid of 1 row and as many columns as there are numerical columns)
    n_cols = len(numerical_columns)
    fig, axes = plt.subplots(1, n_cols, figsize=(15, 6))

    # Set color palette
    colors = sns.color_palette("Set2", n_colors=n_cols)

    # Loop through the numerical columns and create a boxplot for each
    for idx, col in enumerate(numerical_columns):
        sns.boxplot(data=data[col], ax=axes[idx], color=colors[idx])
        axes[idx].set_title(f"{title_prefix} Boxplot for {col}")
        axes[idx].set_xlabel(col)
        axes[idx].set_ylabel('Values')

    # Adjust layout for better visualization
    plt.tight_layout()
    plt.show()

# List of numerical columns in your dataset
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Plot the boxplots for 'before' outlier removal
plot_boxplot(data, numerical_columns, "Before Handling")

"""**Handle Outliers**"""

# Define a function to handle outliers using the IQR method
def handle_outliers(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

# Handle outliers in the numerical columns (for outlier removal)
for col in numerical_columns:
    data = handle_outliers(data, col)

# Plot the boxplots for 'after' outlier removal
plot_boxplot(data, numerical_columns, "After Handling")

"""**Feature Engineering**"""

# # Feature Engineering: Compute Car Depreciation
# data['Car_depreciation'] = (data['Present_Price'] - df['Selling_Price']).round(3)

# Calculate car age from 'Year'
data['Year'] = pd.to_datetime(data['Year'], errors='coerce')  # 'coerce' will handle invalid parsing

# Extract the year from the datetime column
data['Year'] = data['Year'].dt.year

# Now calculate Car Age
data['Car_Age'] = 2025 - data['Year']  # Subtract the car's manufacturing year from the current year

# Drop the 'Year' column as it's no longer needed
data.drop('Year', axis=1, inplace=True)

# Display the updated dataset
print(data.head())

"""**Encode Categorical Columns**


"""

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Assuming the 'Car_Name' format is 'Brand Model', you can extract 'Brand' by splitting the name.
data['Car_Brand'] = data['Car_Name'].apply(lambda x: x.split()[0])  # Splits by space and takes the first word

# Optionally, you can label encode the 'Car_Brand' column if you think it could be relevant.
data['Car_Brand'] = label_encoder.fit_transform(data['Car_Brand'])

# Apply Label Encoding to categorical columns (excluding 'Car_Brand' since it's already encoded)
categorical_columns = data.select_dtypes(include=['object']).columns  # specify categorical columns

# Loop through categorical columns and apply label encoding to each
for col in categorical_columns:
    if col != 'Car_Brand':  # Exclude 'Car_Brand' as it's already encoded
        data[col] = label_encoder.fit_transform(data[col])

# Verify the changes
print("\nData after Label Encoding:")
print(data[categorical_columns].head())

"""**Exploratory Data Analysis (EDA) and Visualization**"""

print(data.columns)

# Set up the figure with multiple subplots
fig, axes = plt.subplots(3, 3, figsize=(18, 15))

# Set up titles for subplots
axes[0, 0].set_title('Price Distribution')
axes[0, 1].set_title('Car Age Distribution')
axes[0, 2].set_title('Car Age vs Price (Scatter)')
axes[1, 0].set_title('Driven Kms vs Price (Scatter)')
axes[1, 1].set_title('Driven Kms vs Car Age (Scatter)')
axes[1, 2].set_title('Selling Price vs Present Price (Scatter)')
axes[2, 0].set_title('Selling Price by Car Brand (Boxplot)')
axes[2, 1].set_title('Fuel Type Distribution')
axes[2, 2].set_title('Correlation Heatmap')

# 1. Price Distribution (Histogram)
sns.histplot(data['Selling_Price'], kde=True, ax=axes[0, 0], color='royalblue', bins=30)
axes[0, 0].set_xlabel('Price')
axes[0, 0].set_ylabel('Frequency')

# 2. Car Age Distribution (Histogram)
sns.histplot(data['Car_Age'], kde=True, ax=axes[0, 1], color='coral', bins=30)
axes[0, 1].set_xlabel('Car Age')
axes[0, 1].set_ylabel('Frequency')

# 3. Scatter Plot: Car Age vs Price
sns.scatterplot(x='Car_Age', y='Selling_Price', data=data, ax=axes[0, 2], color='forestgreen')
axes[0, 2].set_xlabel('Car Age')
axes[0, 2].set_ylabel('Price')

# 4. Scatter Plot: Driven Kms vs Price
sns.scatterplot(x='Driven_kms', y='Selling_Price', data=data, ax=axes[1, 0], color='purple')
axes[1, 0].set_xlabel('Driven Kms')
axes[1, 0].set_ylabel('Price')

# 5. Scatter Plot: Driven Kms vs Car Age
sns.scatterplot(x='Driven_kms', y='Car_Age', data=data, ax=axes[1, 1], color='darkorange')
axes[1, 1].set_xlabel('Driven Kms')
axes[1, 1].set_ylabel('Car Age')

# 6. Scatter Plot: Selling Price vs Present Price
sns.scatterplot(x='Present_Price', y='Selling_Price', data=data, ax=axes[1, 2], color='teal')
axes[1, 2].set_xlabel('Present Price')
axes[1, 2].set_ylabel('Selling Price')

# 7. Boxplot: Selling Price by Car Brand
sns.boxplot(x='Car_Brand', y='Selling_Price', data=data, ax=axes[2, 0], palette='muted')
axes[2, 0].set_xlabel('Car Brand')
axes[2, 0].set_ylabel('Selling Price')

# 8. Fuel Type Distribution (Bar Plot)
sns.countplot(x='Fuel_Type', data=data, ax=axes[2, 1], palette='Set2')
axes[2, 1].set_xlabel('Fuel Type')
axes[2, 1].set_ylabel('Count')

# 9. Correlation Heatmap
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[2, 2], fmt=".2f", linewidths=0.5)

# Adjust layout to avoid overlap
plt.tight_layout()
plt.show()

"""**Interactive Visualization**"""

# Assuming the data is already loaded and processed

# 1. Price Distribution (Histogram)
fig1 = px.histogram(data, x="Selling_Price", nbins=30, title="Price Distribution", color_discrete_sequence=["#1f77b4"])  # Custom color
fig1.update_layout(xaxis_title="Price", yaxis_title="Frequency")
fig1.show()

# 2. Car Age Distribution (Histogram)
fig2 = px.histogram(data, x="Car_Age", nbins=30, title="Car Age Distribution", color_discrete_sequence=["#ff7f0e"])  # Custom color
fig2.update_layout(xaxis_title="Car Age", yaxis_title="Frequency")
fig2.show()

# 3. Car Age vs Price (Scatter)
fig3 = px.scatter(data, x="Car_Age", y="Selling_Price", title="Car Age vs Price", color_discrete_sequence=["#2ca02c"])  # Custom color
fig3.update_layout(xaxis_title="Car Age", yaxis_title="Selling Price")
fig3.show()

# 4. Driven Kms vs Price (Scatter)
fig4 = px.scatter(data, x="Driven_kms", y="Selling_Price", title="Driven Kms vs Price", color_discrete_sequence=["#9467bd"])  # Custom color
fig4.update_layout(xaxis_title="Driven Kms", yaxis_title="Selling Price")
fig4.show()

# 5. Driven Kms vs Car Age (Scatter)
fig5 = px.scatter(data, x="Driven_kms", y="Car_Age", title="Driven Kms vs Car Age", color_discrete_sequence=["#d62728"])  # Custom color
fig5.update_layout(xaxis_title="Driven Kms", yaxis_title="Car Age")
fig5.show()

# 6. Selling Price vs Present Price (Scatter)
fig6 = px.scatter(data, x="Present_Price", y="Selling_Price", title="Selling Price vs Present Price", color_discrete_sequence=["#17becf"])  # Custom color
fig6.update_layout(xaxis_title="Present Price", yaxis_title="Selling Price")
fig6.show()

# 7. Selling Price by Car Brand (Boxplot)
fig7 = px.box(data, x="Car_Brand", y="Selling_Price", title="Selling Price by Car Brand", color="Car_Brand", color_discrete_sequence=px.colors.qualitative.Plotly)  # Updated color sequence
fig7.update_layout(xaxis_title="Car Brand", yaxis_title="Selling Price")
fig7.show()

# 8. Fuel Type Distribution (Bar Plot)
fig8 = px.bar(data, x="Fuel_Type", title="Fuel Type Distribution", color="Fuel_Type", color_discrete_sequence=px.colors.qualitative.Dark24)  # Updated color sequence
fig8.update_layout(xaxis_title="Fuel Type", yaxis_title="Count")
fig8.show()


# 9. Correlation Heatmap
corr_matrix = data.corr()
fig9 = go.Figure(data=go.Heatmap(
    z=corr_matrix.values,
    x=corr_matrix.columns,
    y=corr_matrix.columns,
    colorscale="Viridis",
    zmin=-1, zmax=1,
    colorbar=dict(title="Correlation")
))
fig9.update_layout(title="Correlation Heatmap")
fig9.show()

"""**Feature Selection and Scaling**"""

# Assuming 'data' is your DataFrame and 'Selling_Price' is the target column
X = data.drop('Selling_Price', axis=1)  # Drop the target column 'Selling_Price'
y = data['Selling_Price']  # The target variable 'Selling_Price'

# Apply log transformation to 'Selling_Price' to reduce skewness
y_log = np.log1p(y)  # Log transformation (log(x+1))

# Feature scaling using StandardScaler
scaler = StandardScaler()

# Apply scaling only to numerical features (assuming categorical columns are encoded)
X_scaled = scaler.fit_transform(X)  # StandardScaler automatically scales only numeric columns

# Check the scaled features and transformed target variable
print("Transformed target (log of Selling_Price):")
print(y_log.head())  # Display the first few log-transformed target values

print("\nScaled features (first few rows):")
print(X_scaled[:5])  # Display the first few scaled features

"""**Splitting the Data into Train and Test Sets**"""

data

# Split the data into training and testing sets (80% for training and 20% for testing)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")

"""**Model Selection and Training : Linear Model**"""

# Initialize the model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

"""**Linear Model Evaluation**"""

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Plot actual vs predicted car prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Car Prices')
plt.show()

"""**Check for overfitting**"""

cv_scores = cross_val_score(model, X_scaled, y, cv=5)  # 5-fold cross-validation
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV score: {cv_scores.mean()}")

"""**Lasso Liner Model**"""

model = Lasso(alpha=0.1)  # You can adjust alpha to control regularization strength
model.fit(X_train, y_train)
# Training set predictions
train_pred = model.predict(X_train)
train_mae = mean_absolute_error(y_train, train_pred)
train_mse = mean_squared_error(y_train, train_pred)
train_r2 = r2_score(y_train, train_pred)

# Output the results
print("Training Performance:")
print(f"MAE: {train_mae}")
print(f"MSE: {train_mse}")
print(f"R-squared: {train_r2}")

print("\nTesting Performance:")
print(f"MAE: {val_mae}")
print(f"MSE: {val_mse}")
print(f"R-squared: {val_r2}")

# Use MSE as the evaluation metric
mse_scorer = make_scorer(mean_squared_error)
mse_cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring=mse_scorer)
print(f"Cross-validation MSE scores: {mse_cv_scores}")
print(f"Mean MSE score: {mse_cv_scores.mean()}")

# Plot actual vs predicted car prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Car Prices')
plt.show()

"""**Random Forest Model**"""

#Random Forest Create and fit the model
model = RandomForestRegressor(max_depth=10, min_samples_split=2, n_estimators=50)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Test MAE:", mae)
print("Test MSE:", mse)
print("Test R-squared:", r2)

# # Perform 5-fold cross-validation
# cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

# # Output the mean and standard deviation of the scores
# print("Mean CV score:", cv_scores.mean())
# print("Standard deviation of CV score:", cv_scores.std())

# rf = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=4)

# param_grid = {
#     'n_estimators': [50, 100, 200],
#     'max_depth': [5, 10, 15],
#     'min_samples_split': [2, 4, 6]
# }

# grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
# grid_search.fit(X_train, y_train)
# print(f"Best parameters: {grid_search.best_params_}")

"""**Visualization**"""

# Plot actual vs predicted car prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Car Prices')
plt.show()

# Initialize the Dash app
app = dash.Dash(__name__)

# Layout of the Dash application
app.layout = html.Div([
    html.H1('Car Price Prediction Dashboard'),

    # Input fields for user interaction
    dcc.Input(id='year-input', type='number', placeholder='Enter Year', value=2020),
    dcc.Input(id='present-price-input', type='number', placeholder='Enter Present Price', value=5.0),
    dcc.Input(id='kms-input', type='number', placeholder='Enter Driven Kms', value=10000),
    dcc.Dropdown(
        id='fuel-type-dropdown',
        options=[
            {'label': 'Petrol', 'value': 'Petrol'},
            {'label': 'Diesel', 'value': 'Diesel'},
            {'label': 'CNG', 'value': 'CNG'}
        ],
        value='Petrol',
    ),
    dcc.Dropdown(
        id='transmission-dropdown',
        options=[
            {'label': 'Manual', 'value': 'Manual'},
            {'label': 'Automatic', 'value': 'Automatic'}
        ],
        value='Manual',
    ),
    dcc.Dropdown(
        id='owner-dropdown',
        options=[{'label': str(i), 'value': i} for i in range(1, 6)],
        value=1,
    ),

    html.Button('Predict Price', id='predict-btn', n_clicks=0),
    html.Div(id='prediction-result', style={'fontSize': 20, 'color': 'green', 'fontWeight': 'bold'}),
])

# Callback function for making predictions
@app.callback(
    Output('prediction-result', 'children'),
    [Input('year-input', 'value'),
     Input('present-price-input', 'value'),
     Input('kms-input', 'value'),
     Input('fuel-type-dropdown', 'value'),
     Input('transmission-dropdown', 'value'),
     Input('owner-dropdown', 'value'),
     Input('predict-btn', 'n_clicks')]
)
def predict_price(year, present_price, kms, fuel_type, transmission, owner, n_clicks):
    if n_clicks > 0:
        # Data preprocessing for user input
        user_input = np.array([[year, present_price, kms, owner]])

        # Encoding categorical variables (Fuel Type and Transmission)
        fuel_type_encoded = [1, 0] if fuel_type == 'Petrol' else [0, 1]
        transmission_encoded = 1 if transmission == 'Automatic' else 0

        user_input = np.hstack([user_input, fuel_type_encoded, [transmission_encoded]])

        # Scaling user input
        user_input_scaled = scaler.transform(user_input)

        # Predict the price
        predicted_price = model.predict(user_input_scaled)
        return f'Predicted Selling Price: â‚¹ {predicted_price[0]:,.2f}'

    return ''

# Run the Dash app
if __name__ == '__main__':
    app.run_server(debug=True)